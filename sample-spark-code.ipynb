{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b16eea20-b891-4070-9838-ecc2ba881f8d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pyspark Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b969553d-2dc7-4284-a820-0be88b77c262",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>label1</th><th>label2</th></tr></thead><tbody><tr><td>0</td><td>a</td><td>e</td></tr><tr><td>1</td><td>b</td><td>f</td></tr><tr><td>2</td><td>c</td><td>e</td></tr><tr><td>3</td><td>a</td><td>f</td></tr><tr><td>4</td><td>a</td><td>f</td></tr><tr><td>5</td><td>c</td><td>f</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         "a",
         "e"
        ],
        [
         1,
         "b",
         "f"
        ],
        [
         2,
         "c",
         "e"
        ],
        [
         3,
         "a",
         "f"
        ],
        [
         4,
         "a",
         "f"
        ],
        [
         5,
         "c",
         "f"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "label1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "label2",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create some test data:\n",
    "from pyspark.sql import Row\n",
    "\n",
    "testData = sc.parallelize([\n",
    "    Row(id=0, label1=\"a\", label2=\"e\"),\n",
    "    Row(id=1, label1=\"b\", label2=\"f\"),\n",
    "    Row(id=2, label1=\"c\", label2=\"e\"),\n",
    "    Row(id=3, label1=\"a\", label2=\"f\"),\n",
    "    Row(id=4, label1=\"a\", label2=\"f\"),\n",
    "    Row(id=5, label1=\"c\", label2=\"f\")\n",
    "], 3)\n",
    "df = spark.createDataFrame(testData)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3606a3ec-ad79-4f91-b6de-9b924060863f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0786415d-44d3-4efa-acff-939cecacbee7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0;31mType:\u001B[0m           ABCMeta\n\u001B[0;31mString form:\u001B[0m    abc.ABCMeta instance\n\u001B[0;31mFile:\u001B[0m           /databricks/spark/python/pyspark/ml/feature.py\n\u001B[0;31mLine:\u001B[0m           4521\n\u001B[0;31mDocstring:\u001B[0m     \nA label indexer that maps a string column of labels to an ML column of label indices.\nIf the input column is numeric, we cast it to string and index the string values.\nThe indices are in [0, numLabels). By default, this is ordered by label frequencies\nso the most frequent label gets index 0. The ordering behavior is controlled by\nsetting :py:attr:`stringOrderType`. Its default value is 'frequencyDesc'.\n\n.. versionadded:: 1.4.0\n\nExamples\n--------\n>>> stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\",\n...     stringOrderType=\"frequencyDesc\")\n>>> stringIndexer.setHandleInvalid(\"error\")\nStringIndexer...\n>>> model = stringIndexer.fit(stringIndDf)\n>>> model.setHandleInvalid(\"error\")\nStringIndexerModel...\n>>> td = model.transform(stringIndDf)\n>>> sorted(set([(i[0], i[1]) for i in td.select(td.id, td.indexed).collect()]),\n...     key=lambda x: x[0])\n[(0, 0.0), (1, 2.0), (2, 1.0), (3, 0.0), (4, 0.0), (5, 1.0)]\n>>> inverter = IndexToString(inputCol=\"indexed\", outputCol=\"label2\", labels=model.labels)\n>>> itd = inverter.transform(td)\n>>> sorted(set([(i[0], str(i[1])) for i in itd.select(itd.id, itd.label2).collect()]),\n...     key=lambda x: x[0])\n[(0, 'a'), (1, 'b'), (2, 'c'), (3, 'a'), (4, 'a'), (5, 'c')]\n>>> stringIndexerPath = temp_path + \"/string-indexer\"\n>>> stringIndexer.save(stringIndexerPath)\n>>> loadedIndexer = StringIndexer.load(stringIndexerPath)\n>>> loadedIndexer.getHandleInvalid() == stringIndexer.getHandleInvalid()\nTrue\n>>> modelPath = temp_path + \"/string-indexer-model\"\n>>> model.save(modelPath)\n>>> loadedModel = StringIndexerModel.load(modelPath)\n>>> loadedModel.labels == model.labels\nTrue\n>>> indexToStringPath = temp_path + \"/index-to-string\"\n>>> inverter.save(indexToStringPath)\n>>> loadedInverter = IndexToString.load(indexToStringPath)\n>>> loadedInverter.getLabels() == inverter.getLabels()\nTrue\n>>> loadedModel.transform(stringIndDf).take(1) == model.transform(stringIndDf).take(1)\nTrue\n>>> stringIndexer.getStringOrderType()\n'frequencyDesc'\n>>> stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed\", handleInvalid=\"error\",\n...     stringOrderType=\"alphabetDesc\")\n>>> model = stringIndexer.fit(stringIndDf)\n>>> td = model.transform(stringIndDf)\n>>> sorted(set([(i[0], i[1]) for i in td.select(td.id, td.indexed).collect()]),\n...     key=lambda x: x[0])\n[(0, 2.0), (1, 1.0), (2, 0.0), (3, 2.0), (4, 2.0), (5, 0.0)]\n>>> fromlabelsModel = StringIndexerModel.from_labels([\"a\", \"b\", \"c\"],\n...     inputCol=\"label\", outputCol=\"indexed\", handleInvalid=\"error\")\n>>> result = fromlabelsModel.transform(stringIndDf)\n>>> sorted(set([(i[0], i[1]) for i in result.select(result.id, result.indexed).collect()]),\n...     key=lambda x: x[0])\n[(0, 0.0), (1, 1.0), (2, 2.0), (3, 0.0), (4, 0.0), (5, 2.0)]\n>>> testData = sc.parallelize([Row(id=0, label1=\"a\", label2=\"e\"),\n...                            Row(id=1, label1=\"b\", label2=\"f\"),\n...                            Row(id=2, label1=\"c\", label2=\"e\"),\n...                            Row(id=3, label1=\"a\", label2=\"f\"),\n...                            Row(id=4, label1=\"a\", label2=\"f\"),\n...                            Row(id=5, label1=\"c\", label2=\"f\")], 3)\n>>> multiRowDf = spark.createDataFrame(testData)\n>>> inputs = [\"label1\", \"label2\"]\n>>> outputs = [\"index1\", \"index2\"]\n>>> stringIndexer = StringIndexer(inputCols=inputs, outputCols=outputs)\n>>> model = stringIndexer.fit(multiRowDf)\n>>> result = model.transform(multiRowDf)\n>>> sorted(set([(i[0], i[1], i[2]) for i in result.select(result.id, result.index1,\n...     result.index2).collect()]), key=lambda x: x[0])\n[(0, 0.0, 1.0), (1, 2.0, 0.0), (2, 1.0, 1.0), (3, 0.0, 0.0), (4, 0.0, 0.0), (5, 1.0, 0.0)]\n>>> fromlabelsModel = StringIndexerModel.from_arrays_of_labels([[\"a\", \"b\", \"c\"], [\"e\", \"f\"]],\n...     inputCols=inputs, outputCols=outputs)\n>>> result = fromlabelsModel.transform(multiRowDf)\n>>> sorted(set([(i[0], i[1], i[2]) for i in result.select(result.id, result.index1,\n...     result.index2).collect()]), key=lambda x: x[0])\n[(0, 0.0, 0.0), (1, 1.0, 1.0), (2, 2.0, 0.0), (3, 0.0, 1.0), (4, 0.0, 1.0), (5, 2.0, 1.0)]\n\u001B[0;31mInit docstring:\u001B[0m __init__(self, \\*, inputCol=None, outputCol=None, inputCols=None, outputCols=None,                  handleInvalid=\"error\", stringOrderType=\"frequencyDesc\")"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "StringIndexer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9faad4c-de1a-48b8-b878-7c0578de7353",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>label1</th><th>label2</th><th>index1</th><th>index2</th></tr></thead><tbody><tr><td>0</td><td>a</td><td>e</td><td>0.0</td><td>1.0</td></tr><tr><td>1</td><td>b</td><td>f</td><td>2.0</td><td>0.0</td></tr><tr><td>2</td><td>c</td><td>e</td><td>1.0</td><td>1.0</td></tr><tr><td>3</td><td>a</td><td>f</td><td>0.0</td><td>0.0</td></tr><tr><td>4</td><td>a</td><td>f</td><td>0.0</td><td>0.0</td></tr><tr><td>5</td><td>c</td><td>f</td><td>1.0</td><td>0.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         "a",
         "e",
         0.0,
         1.0
        ],
        [
         1,
         "b",
         "f",
         2.0,
         0.0
        ],
        [
         2,
         "c",
         "e",
         1.0,
         1.0
        ],
        [
         3,
         "a",
         "f",
         0.0,
         0.0
        ],
        [
         4,
         "a",
         "f",
         0.0,
         0.0
        ],
        [
         5,
         "c",
         "f",
         1.0,
         0.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "label1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "label2",
         "type": "\"string\""
        },
        {
         "metadata": "{\"ml_attr\":{\"vals\":[\"a\",\"c\",\"b\"],\"type\":\"nominal\",\"name\":\"index1\"}}",
         "name": "index1",
         "type": "\"double\""
        },
        {
         "metadata": "{\"ml_attr\":{\"vals\":[\"f\",\"e\"],\"type\":\"nominal\",\"name\":\"index2\"}}",
         "name": "index2",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Note you can use `inputCols`/`outputCols` instead of `inputCol`/`outputCol` to do multiple columns at once.\n",
    "inputs = [\"label1\", \"label2\"]\n",
    "outputs = [\"index1\", \"index2\"]\n",
    "stringIndexer = StringIndexer(inputCols=inputs, outputCols=outputs)\n",
    "\n",
    "# Make the transformations:\n",
    "model = stringIndexer.fit(df)\n",
    "result = model.transform(df)\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "976a6885-ea69-4b23-bd39-6b13ee65f9d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "876d9f6b-c7b6-42cd-9e18-5d4934644d2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0;31mType:\u001B[0m           ABCMeta\n\u001B[0;31mString form:\u001B[0m    abc.ABCMeta instance\n\u001B[0;31mFile:\u001B[0m           /databricks/spark/python/pyspark/ml/feature.py\n\u001B[0;31mLine:\u001B[0m           3080\n\u001B[0;31mDocstring:\u001B[0m     \nA one-hot encoder that maps a column of category indices to a column of binary vectors, with\nat most a single one-value per row that indicates the input category index.\nFor example with 5 categories, an input value of 2.0 would map to an output vector of\n`[0.0, 0.0, 1.0, 0.0]`.\nThe last category is not included by default (configurable via :py:attr:`dropLast`),\nbecause it makes the vector entries sum up to one, and hence linearly dependent.\nSo an input value of 4.0 maps to `[0.0, 0.0, 0.0, 0.0]`.\n\nWhen :py:attr:`handleInvalid` is configured to 'keep', an extra \"category\" indicating invalid\nvalues is added as last category. So when :py:attr:`dropLast` is true, invalid values are\nencoded as all-zeros vector.\n\n.. versionadded:: 2.3.0\n\nNotes\n-----\nThis is different from scikit-learn's OneHotEncoder, which keeps all categories.\nThe output vectors are sparse.\n\nWhen encoding multi-column by using :py:attr:`inputCols` and\n:py:attr:`outputCols` params, input/output cols come in pairs, specified by the order in\nthe arrays, and each pair is treated independently.\n\nSee Also\n--------\nStringIndexer : for converting categorical values into category indices\n\nExamples\n--------\n>>> from pyspark.ml.linalg import Vectors\n>>> df = spark.createDataFrame([(0.0,), (1.0,), (2.0,)], [\"input\"])\n>>> ohe = OneHotEncoder()\n>>> ohe.setInputCols([\"input\"])\nOneHotEncoder...\n>>> ohe.setOutputCols([\"output\"])\nOneHotEncoder...\n>>> model = ohe.fit(df)\n>>> model.setOutputCols([\"output\"])\nOneHotEncoderModel...\n>>> model.getHandleInvalid()\n'error'\n>>> model.transform(df).head().output\nSparseVector(2, {0: 1.0})\n>>> single_col_ohe = OneHotEncoder(inputCol=\"input\", outputCol=\"output\")\n>>> single_col_model = single_col_ohe.fit(df)\n>>> single_col_model.transform(df).head().output\nSparseVector(2, {0: 1.0})\n>>> ohePath = temp_path + \"/ohe\"\n>>> ohe.save(ohePath)\n>>> loadedOHE = OneHotEncoder.load(ohePath)\n>>> loadedOHE.getInputCols() == ohe.getInputCols()\nTrue\n>>> modelPath = temp_path + \"/ohe-model\"\n>>> model.save(modelPath)\n>>> loadedModel = OneHotEncoderModel.load(modelPath)\n>>> loadedModel.categorySizes == model.categorySizes\nTrue\n>>> loadedModel.transform(df).take(1) == model.transform(df).take(1)\nTrue\n\u001B[0;31mInit docstring:\u001B[0m __init__(self, \\*, inputCols=None, outputCols=None, handleInvalid=\"error\", dropLast=True,                  inputCol=None, outputCol=None)"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "OneHotEncoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07b0620-3f84-413f-9613-60b5db18ef57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53730333f18f44e9a049272eb0f94305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9aee867a5e4f1ab5d3c730a538a143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>label1</th><th>label2</th><th>index1</th><th>index2</th><th>ohe1</th><th>ohe2</th></tr></thead><tbody><tr><td>0</td><td>a</td><td>e</td><td>0.0</td><td>1.0</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(1), values -> List(1.0))</td></tr><tr><td>1</td><td>b</td><td>f</td><td>2.0</td><td>0.0</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(2), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(0), values -> List(1.0))</td></tr><tr><td>2</td><td>c</td><td>e</td><td>1.0</td><td>1.0</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(1), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(1), values -> List(1.0))</td></tr><tr><td>3</td><td>a</td><td>f</td><td>0.0</td><td>0.0</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(0), values -> List(1.0))</td></tr><tr><td>4</td><td>a</td><td>f</td><td>0.0</td><td>0.0</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(0), values -> List(1.0))</td></tr><tr><td>5</td><td>c</td><td>f</td><td>1.0</td><td>0.0</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(1), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 3, indices -> List(0), values -> List(1.0))</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         "a",
         "e",
         0.0,
         1.0,
         {
          "indices": [
           0
          ],
          "length": 4,
          "values": [
           1.0
          ],
          "vectorType": "sparse"
         },
         {
          "indices": [
           1
          ],
          "length": 3,
          "values": [
           1.0
          ],
          "vectorType": "sparse"
         }
        ],
        [
         1,
         "b",
         "f",
         2.0,
         0.0,
         {
          "indices": [
           2
          ],
          "length": 4,
          "values": [
           1.0
          ],
          "vectorType": "sparse"
         },
         {
          "indices": [
           0
          ],
          "length": 3,
          "values": [
           1.0
          ],
          "vectorType": "sparse"
         }
        ],
        [
         2,
         "c",
         "e",
         1.0,
         1.0,
         {
          "indices": [
           1
          ],
          "length": 4,
          "values": [
           1.0
          ],
          "vectorType": "sparse"
         },
         {
          "indices": [
           1
          ],
          "length": 3,
          "values": [
           1.0
          ],
          "vectorType": "sparse"
         }
        ],
        [
         3,
         "a",
         "f",
         0.0,
         0.0,
         {
          "indices": [
           0
          ],
          "length": 4,
          "values": [
           1.0
          ],
          "vectorType": "sparse"
         },
         {
          "indices": [
           0
          ],
          "length": 3,
          "values": [
           1.0
          ],
          "vectorType": "sparse"
         }
        ],
        [
         4,
         "a",
         "f",
         0.0,
         0.0,
         {
          "indices": [
           0
          ],
          "length": 4,
          "values": [
           1.0
          ],
          "vectorType": "sparse"
         },
         {
          "indices": [
           0
          ],
          "length": 3,
          "values": [
           1.0
          ],
          "vectorType": "sparse"
         }
        ],
        [
         5,
         "c",
         "f",
         1.0,
         0.0,
         {
          "indices": [
           1
          ],
          "length": 4,
          "values": [
           1.0
          ],
          "vectorType": "sparse"
         },
         {
          "indices": [
           0
          ],
          "length": 3,
          "values": [
           1.0
          ],
          "vectorType": "sparse"
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "label1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "label2",
         "type": "\"string\""
        },
        {
         "metadata": "{\"ml_attr\":{\"vals\":[\"a\",\"c\",\"b\",\"__unknown\"],\"type\":\"nominal\",\"name\":\"index1\"}}",
         "name": "index1",
         "type": "\"double\""
        },
        {
         "metadata": "{\"ml_attr\":{\"vals\":[\"f\",\"e\",\"__unknown\"],\"type\":\"nominal\",\"name\":\"index2\"}}",
         "name": "index2",
         "type": "\"double\""
        },
        {
         "metadata": "{\"ml_attr\":{\"attrs\":{\"binary\":[{\"idx\":0,\"name\":\"a\"},{\"idx\":1,\"name\":\"c\"},{\"idx\":2,\"name\":\"b\"},{\"idx\":3,\"name\":\"__unknown\"}]},\"num_attrs\":4}}",
         "name": "ohe1",
         "type": "{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}"
        },
        {
         "metadata": "{\"ml_attr\":{\"attrs\":{\"binary\":[{\"idx\":0,\"name\":\"f\"},{\"idx\":1,\"name\":\"e\"},{\"idx\":2,\"name\":\"__unknown\"}]},\"num_attrs\":3}}",
         "name": "ohe2",
         "type": "{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Need to convert string columns to numeric values before One Hot Encoding.\n",
    "indexer_inputs = [\"label1\", \"label2\"]\n",
    "indexer_outputs = [\"index1\", \"index2\"]\n",
    "indexer = StringIndexer(\n",
    "    inputCols=indexer_inputs, \n",
    "    outputCols=indexer_outputs,\n",
    "    handleInvalid='keep'\n",
    ")\n",
    "\n",
    "# Note you can use `inputCols`/`outputCols` instead of `inputCol`/`outputCol` to do multiple columns at once.\n",
    "ohe_inputs = [\"index1\", \"index2\"]  # Note these are the StringIndexer outputs.\n",
    "ohe_outputs = [\"ohe1\", \"ohe2\"]\n",
    "ohe = OneHotEncoder(\n",
    "    inputCols=ohe_inputs, \n",
    "    outputCols=ohe_outputs, \n",
    "    handleInvalid='keep', \n",
    "    dropLast=True\n",
    ")\n",
    "\n",
    "# Make a pipeline to do the 2 stages in sequence:\n",
    "from pyspark.ml import Pipeline\n",
    "pipe = Pipeline(stages=[indexer, ohe])\n",
    "\n",
    "# Make the transformations:\n",
    "model = pipe.fit(df)\n",
    "result = model.transform(df)\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5107e18d-f659-447f-a88d-de45b84e9420",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sample-spark-code",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
